{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from model import TimmModel\n",
    "model = TimmModel('vit_base_patch16_clip_224.openai',\n",
    "                        pretrained=True,\n",
    "                        img_size=384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n",
      "torch.Size([1, 768]) torch.Size([1, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "rand_img = torch.rand(1, 3, 384, 384)\n",
    "feature_1 = model(None, rand_img)\n",
    "print(feature_1.shape)\n",
    "rand_another_img = torch.rand(1, 3, 384, 384 * 2)\n",
    "feature_1, feature_2 = model(rand_another_img, rand_img)\n",
    "print(feature_1.shape, feature_2.shape)\n",
    "feature_2 = model(rand_another_img)\n",
    "print(feature_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation, SegformerConfig\n",
    "\n",
    "\n",
    "segformer_model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b3-finetuned-cityscapes-1024-1024')\n",
    "model = segformer_model.base_model.encoder  # Extract the encoder\n",
    "\n",
    "\n",
    "\n",
    "# Define a projection layer to map to desired output_dim\n",
    "# We'll first apply adaptive pooling to reduce spatial dimensions, then flatten and project\n",
    "from torch import nn\n",
    "projection = nn.Sequential(\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),  # Pool spatial dimensions to 1x1\n",
    "    nn.Flatten(),                   # Flatten to (batch_size, hidden_channels)\n",
    "    nn.Linear(512, 1024),  # Project to 1024\n",
    "    nn.ReLU()                       # Optional non-linearity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 12, 12])\n",
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "feature_1 = model(rand_img).last_hidden_state\n",
    "print(feature_1.shape)\n",
    "feature_1 = projection(feature_1)\n",
    "print(feature_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 128, 320, 512]\n"
     ]
    }
   ],
   "source": [
    "print(segformer_model.config.hidden_sizes)  # 512 for segformer-b3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP-LoRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
