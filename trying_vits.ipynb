{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_scriptable, is_exportable, set_scriptable, set_exportable\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model, list_models, list_pretrained, is_model, list_modules, model_entrypoint, \\\n\u001b[1;32m      4\u001b[0m     is_model_pretrained, get_pretrained_cfg, get_pretrained_cfg_value\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbeit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbyoanet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbyobnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/models/beit.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PatchEmbed, Mlp, SwiGLU, LayerNorm, DropPath, trunc_normal_, use_fused_attn\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resample_patch_embed, resample_abs_pos_embed, resize_rel_pos_bias_table, ndgrid\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/data/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resolve_data_config, resolve_model_data_config\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataset, IterableImageDataset, AugMixDataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_dataset\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, CustomDatasetInfo\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/data/dataset.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_reader\n\u001b[1;32m     15\u001b[0m _logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     18\u001b[0m _ERROR_RETRY \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/data/readers/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_reader\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimg_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/data/readers/reader_factory.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader_image_folder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReaderImageFolder\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreader_image_in_tar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReaderImageInTar\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_reader\u001b[39m(\n\u001b[1;32m      9\u001b[0m         name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     10\u001b[0m         root: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m         split: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     13\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/data/readers/reader_image_folder.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Optional, Set, Tuple, Union\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m natural_key\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_map\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_class_map\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimg_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_img_extensions\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/utils/__init__.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_ema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelEma, ModelEmaV2, ModelEmaV3\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_seed\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m update_summary, get_outdir\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/timm/utils/summary.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# This needs to be early as other modules call it.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mterm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m termsetup, termlog, termerror, termwarn\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdk \u001b[38;5;28;01mas\u001b[39;00m wandb_sdk\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     25\u001b[0m wandb\u001b[38;5;241m.\u001b[39mwandb_lib \u001b[38;5;241m=\u001b[39m wandb_sdk\u001b[38;5;241m.\u001b[39mlib  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/sdk/__init__.py:25\u001b[0m\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSettings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhelper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wandb_helper \u001b[38;5;28;01mas\u001b[39;00m helper\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifacts\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01martifact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_alerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertLevel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwandb_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/sdk/artifacts/artifact.py:37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_types, env, util\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnormalize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize_exceptions\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapis\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArtifactCollection, ArtifactFiles, RetryingClient, Run\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WBValue\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/apis/__init__.py:43\u001b[0m\n\u001b[1;32m     38\u001b[0m     _disable_ssl()\n\u001b[1;32m     41\u001b[0m reset_path \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvendor_setup()\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpublic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m PublicApi  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     46\u001b[0m reset_path()\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/apis/internal.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Api \u001b[38;5;28;01mas\u001b[39;00m InternalApi\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mApi\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Internal proxy to the official internal API.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/sdk/internal/internal_api.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb_gql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, gql\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb_gql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetryError\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gql\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgql\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClient\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/gql-0.2.0/wandb_gql/gql.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb_graphql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwandb_graphql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgql\u001b[39m(request_string):\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/graphql-core-1.1/wandb_graphql/__init__.py:43\u001b[0m\n\u001b[1;32m     38\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m get_version(VERSION)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __GRAPHQL_SETUP__:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# The primary entry point into fulfilling a GraphQL request.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     44\u001b[0m         graphql\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Create and operate on GraphQL type definitions and schema.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# no import order\u001b[39;00m\n\u001b[1;32m     49\u001b[0m         GraphQLSchema,\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m         get_named_type,\n\u001b[1;32m    115\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/graphql-core-1.1/wandb_graphql/graphql.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msource\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Source\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# This is the primary entry point function for fulfilling GraphQL operations\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# by parsing, validating, and executing a GraphQL document along side a\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# GraphQL schema.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#    possible operations. Can be omitted if requestString contains only\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#    one operation.\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgraphql\u001b[39m(schema, request_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, root_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, context_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m             variable_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, executor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m             return_promise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, middleware\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/graphql-core-1.1/wandb_graphql/validation/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m specified_rules\n\u001b[1;32m      4\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecified_rules\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/graphql-core-1.1/wandb_graphql/validation/validation.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphQLSchema\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype_info\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeInfo\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m specified_rules\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(schema, ast, rules\u001b[38;5;241m=\u001b[39mspecified_rules):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m schema, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMust provide schema\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/wandb/vendor/graphql-core-1.1/wandb_graphql/validation/rules/__init__.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munique_argument_names\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniqueArgumentNames\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munique_fragment_names\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniqueFragmentNames\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munique_input_field_names\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniqueInputFieldNames\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munique_operation_names\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniqueOperationNames\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01munique_variable_names\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UniqueVariableNames\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPVisionModel, CLIPProcessor, CLIPVisionConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch.nn.functional as F\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_hf = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch16\", hidden_act = \"gelu\")\n",
    "clip_timm = timm.create_model(\"vit_base_patch16_clip_224.openai\", pretrained=True, num_classes=0, img_size=384)\n",
    "clip_timm_state_dict = clip_timm.state_dict()\n",
    "clip_hf_state_dict = clip_hf.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.7209e-02, -5.7008e-01,  1.7252e-01, -1.9447e-03,  1.0032e+00,\n",
      "         2.3552e-02, -3.1327e-02, -1.6213e-02, -1.2675e-02, -1.6176e-01,\n",
      "         3.6407e-01, -2.0450e-01,  4.7865e-01,  3.5444e-01,  4.0919e-01,\n",
      "         1.9796e-02, -1.4939e-01,  6.0754e-03,  3.4549e-03, -4.4616e-02,\n",
      "         9.6039e-03,  1.0068e-02,  3.5203e-01, -1.5700e-03, -2.7305e-02,\n",
      "        -4.1719e-01, -9.7703e-03,  1.9158e-02,  2.8592e-03,  1.6418e-03,\n",
      "         1.1876e-02, -3.6110e-01,  3.4315e-03,  2.1195e-01, -3.3260e-01,\n",
      "        -2.4247e-01, -1.5742e-01,  1.2279e-02,  4.8328e-01, -1.0050e-01,\n",
      "         2.8371e-02, -6.9131e-02,  7.4177e-03,  3.9833e-01,  7.0305e-02,\n",
      "        -3.8821e-01,  3.1074e-03, -3.9902e-01, -2.7748e-01,  7.2899e-03,\n",
      "        -1.5799e-02,  1.3198e-02,  3.1929e-01,  3.9349e-03, -2.9630e-02,\n",
      "        -3.7958e-03,  3.2401e-01, -2.8247e-01, -4.5286e-04,  4.0700e-03,\n",
      "        -2.2941e-01, -1.0161e-02, -3.7380e-01, -2.4229e-01,  1.6912e-01,\n",
      "         7.7919e-03,  2.5575e-02,  3.4321e-01,  5.8983e-02,  8.7041e-03,\n",
      "         8.6387e-03, -2.8615e-01,  1.4049e-01, -6.9530e-02, -2.5689e-02,\n",
      "         8.6861e-03,  3.8805e-03,  4.1322e-01,  6.0722e-03,  1.2101e-01,\n",
      "         4.3213e-03,  1.4451e-02,  1.1970e-05,  1.2344e-02, -1.4839e-02,\n",
      "        -2.8091e-01, -5.5582e-04,  1.6709e-02,  9.8234e-03,  1.7198e-03,\n",
      "        -7.8114e-02,  7.3818e-03,  5.0359e-02,  4.1475e-03,  8.7621e-03,\n",
      "        -4.1586e-01,  3.2938e-04,  9.4677e-03, -3.1265e-01,  1.9122e-03,\n",
      "         4.2110e-01,  1.5032e-01,  6.4615e-03,  4.3733e-01, -1.6108e-03,\n",
      "         4.9281e-01,  2.3122e-02,  1.1459e-01,  4.1135e-04,  4.7661e-01,\n",
      "         3.5777e-02,  4.7339e-01,  1.6235e-03,  1.1110e-03, -3.5776e-02,\n",
      "        -1.8272e-01,  1.4231e-02,  1.7698e-03,  1.6046e-04,  1.8287e-02,\n",
      "        -2.0778e-03,  6.7395e-01,  6.4461e-03, -3.6283e-01,  1.1954e-02,\n",
      "        -3.7560e-01,  2.2705e-03, -2.8179e-01,  1.7413e-02, -2.4918e-01,\n",
      "        -6.6592e-03, -4.3028e-01,  3.4625e-03, -1.2824e-02, -2.6799e-01,\n",
      "         4.3592e-01,  6.1942e-03,  6.6100e-03,  1.3426e-02, -2.4855e-02,\n",
      "         2.2952e-01, -6.7899e-02, -7.6912e-03,  3.5621e-02, -2.2930e-01,\n",
      "        -3.2746e-03,  3.3737e-01,  1.0726e-02, -1.6076e-02,  3.5266e-03,\n",
      "         1.2606e-02, -3.3668e-02,  8.7779e-03, -2.7328e-01,  9.4028e-03,\n",
      "         1.1265e-02, -2.4634e-02, -4.6336e-03, -6.3863e-03,  6.2576e-03,\n",
      "         1.3848e-01, -1.2891e-01, -1.5537e+00,  1.2133e-01, -6.0076e-02,\n",
      "         1.0747e-02,  7.4486e-03,  2.4497e-01, -1.8637e-02,  5.9198e-03,\n",
      "        -3.1660e-01, -2.8404e-01, -8.7388e-03, -3.7747e-01, -1.5215e-02,\n",
      "        -2.3250e-01,  2.0439e-03,  5.0115e-04,  4.5193e-01,  5.9048e-03,\n",
      "         3.5465e-01,  3.8306e-01,  4.0577e-03,  3.6637e-01,  2.1423e-01,\n",
      "         8.3901e-03,  1.7482e-03,  4.0119e-01,  4.6774e-01,  2.5480e-02,\n",
      "        -6.8499e-02,  4.1738e-01,  2.2604e-01,  3.8059e-01,  5.8841e-03,\n",
      "        -3.6944e-01, -6.6759e-02, -4.3566e-01,  1.9618e-02,  8.3956e-03,\n",
      "        -4.7460e-01, -7.5817e-03,  5.2860e-03,  3.1420e-03,  8.3503e-04,\n",
      "         7.6339e-03,  2.4740e-01,  2.9868e-03, -7.6897e-02,  2.2088e-02,\n",
      "        -2.5523e-01, -4.9735e-01, -5.0655e-02, -4.6392e-01,  1.4133e-03,\n",
      "        -3.5439e-01, -2.7990e-01, -8.2220e-02, -2.5869e-01,  4.2214e-01,\n",
      "         1.2962e-02, -8.5387e-04, -2.4513e-02,  1.2231e-02,  8.9681e-02,\n",
      "        -3.9993e-03,  2.4617e-03, -6.7791e-03,  5.0579e-03, -3.5819e-01,\n",
      "         5.3457e-03,  3.5854e-01,  4.4830e-01,  2.9304e-01,  5.4311e-04,\n",
      "        -4.2298e-01, -3.9505e-01,  1.4610e-02,  3.8553e-03, -5.2851e-03,\n",
      "         1.6204e-01,  8.0612e-03,  8.0376e-03,  1.4297e-03,  5.3434e-03,\n",
      "        -4.2956e-02, -8.7474e-02, -4.2127e-01,  1.7239e-04,  3.7805e-01,\n",
      "         1.1972e-02, -3.7456e-01,  3.0025e-01,  1.1856e-01,  4.1934e-01,\n",
      "        -1.8370e-01, -1.0854e-02, -3.1013e-01, -1.4721e-02,  2.3574e-01,\n",
      "        -1.7547e-02,  2.6636e-02,  8.9467e-02, -2.6821e-01, -4.7130e-03,\n",
      "         5.2529e-03,  6.6395e-03, -2.6224e-01, -4.0888e-01,  5.5415e-01,\n",
      "         2.3556e-01, -1.4716e-01, -5.4020e-01,  2.5845e-03,  3.1552e-03,\n",
      "         1.0061e-02,  1.4213e-02,  4.5550e-02,  2.4636e-01, -3.2381e-01,\n",
      "         7.7000e-03,  1.4330e-02, -1.5107e-02,  2.8264e-01,  5.4835e-01,\n",
      "         1.9124e-01,  1.3763e-02, -6.7660e-03, -9.3073e-03,  9.3380e-03,\n",
      "         3.3279e-01,  1.0401e-03, -1.6795e-03,  1.3305e-02, -9.5019e-02,\n",
      "         7.9593e-03,  3.9392e-03,  2.7991e-02, -3.3914e-01,  2.1478e-03,\n",
      "        -2.8234e-01,  4.0000e-01, -3.1629e-01,  5.2064e-03, -3.0863e-01,\n",
      "         4.3077e-03,  1.7124e-02, -1.0643e-01,  2.2688e-02, -2.3652e-03,\n",
      "         5.4857e-03,  1.8332e-01,  1.3066e-02,  6.3349e-01,  9.5900e-03,\n",
      "         1.9481e-01, -3.3045e-01, -2.8378e-01,  3.3372e-03, -3.1896e-01,\n",
      "        -4.9838e-01, -4.3225e-02, -4.5054e-01,  8.6169e-03, -4.3524e-01,\n",
      "         4.0580e-03,  6.7421e-02, -3.3513e-01,  1.1705e-02,  1.0529e-01,\n",
      "         3.7387e-01,  2.0957e-01,  4.2680e-03, -1.5940e-01,  1.0806e-02,\n",
      "        -1.3836e-03,  3.5585e-03, -2.6658e-01, -4.3860e-03,  3.5356e-03,\n",
      "         4.2298e-01,  3.1948e-01,  3.5632e-02, -1.2766e-02,  4.4848e-01,\n",
      "        -3.6842e-01, -9.3655e-03,  4.1365e-01, -1.1752e-01, -2.9804e-03,\n",
      "         4.6331e-02,  4.4625e-01, -3.1980e-01, -4.9937e-03,  1.0693e-02,\n",
      "         4.7284e-03, -4.4807e-02,  3.5368e-03, -1.1446e-01, -1.8581e-01,\n",
      "        -2.9040e-01,  3.4654e-01,  2.7736e-02, -1.7870e-01,  2.5435e-04,\n",
      "        -2.1284e-01,  3.3768e-01,  1.9170e-02,  6.4218e-03,  3.6997e-02,\n",
      "         2.9352e-03, -2.1867e-01,  5.8905e-03,  3.7788e-03,  1.2010e-02,\n",
      "         8.2617e-03, -1.4471e-01, -4.4483e-03,  4.1717e-01, -1.9383e-02,\n",
      "         9.6542e-04, -1.3193e-04,  6.1482e-03,  1.9696e-03, -4.4972e-01,\n",
      "        -3.7933e-03,  9.0354e-03, -3.7821e-01,  9.9601e-02, -4.5027e-04,\n",
      "        -3.1600e-01,  4.0432e-02, -3.2853e-01, -3.5562e-02,  1.2438e-01,\n",
      "         7.0363e-03,  2.1569e-02, -1.0308e-01, -6.8448e-02,  5.8183e-03,\n",
      "         5.7580e-03,  2.1333e-02,  9.6013e-04, -1.0065e-02,  8.7419e-03,\n",
      "        -3.2981e-05,  4.8738e-01,  2.6715e-01,  3.8944e-04,  5.4350e-01,\n",
      "         5.7291e-01, -9.6718e-03, -1.4296e-01,  1.0543e-01,  1.4396e-02,\n",
      "         4.3829e-03,  5.2188e-03, -3.3270e-01, -7.1623e-03,  3.0989e-02,\n",
      "        -6.3077e-02,  8.4937e-03, -2.9535e-01, -3.0482e-02, -9.6739e-03,\n",
      "         4.1360e-01,  2.2486e-03, -3.8869e-01,  1.3115e-02,  9.7661e-01,\n",
      "         6.0808e-02,  1.9318e-01,  1.7528e-03,  1.0446e-02, -1.8814e-02,\n",
      "        -2.5829e-03,  2.6753e-02, -1.6773e-02, -3.8797e-01,  3.0209e-02,\n",
      "         5.8568e-03, -2.8913e-01, -1.2895e-05,  2.2246e-02,  1.2976e-01,\n",
      "         5.3070e-02,  3.9125e-01,  1.0076e-01, -4.6242e-03,  3.3538e-02,\n",
      "         6.2694e-03, -4.4934e-01,  5.5262e-03,  7.6566e-04, -9.2407e-05,\n",
      "        -1.0811e-02,  3.7944e-04,  6.1506e-04, -1.4525e-02,  6.6671e-03,\n",
      "         2.1837e-04, -3.4198e-04, -1.7039e-03, -5.6206e-02, -5.0602e-02,\n",
      "         3.8434e-03,  2.6947e-01, -3.5124e-01, -5.0148e-03,  6.2534e-03,\n",
      "         4.5156e-01,  1.2791e-02,  2.7110e-02, -5.9018e-03,  4.2692e-01,\n",
      "         2.0360e-01,  1.6827e-02,  1.5986e-02,  3.0807e-01,  6.0206e-01,\n",
      "        -6.3007e-02,  2.8250e-01,  4.0483e-01, -5.2749e-03,  4.2270e-01,\n",
      "         8.1791e-03, -6.2506e-03,  3.4000e-02,  4.0202e-03,  4.7591e-01,\n",
      "        -1.7815e-03, -5.7022e-03, -5.8992e-03,  2.9111e-04,  3.2199e-01,\n",
      "         3.9326e-01, -2.9738e-03,  4.2645e-04,  8.3516e-03,  2.6811e-01,\n",
      "         4.2667e-03,  7.2611e-03, -3.5387e-01,  1.9313e-03, -3.0769e-03,\n",
      "        -2.4823e-01, -3.8411e-02, -8.3992e-02, -5.9510e-03,  1.2335e-02,\n",
      "        -2.2880e-01,  8.7755e-03, -2.2040e-03,  6.5197e-01, -3.0416e-01,\n",
      "         2.7271e-03, -6.5202e-03, -2.5392e-02, -4.6386e-01,  4.8087e-03,\n",
      "         8.9364e-03,  5.3590e-01, -2.2904e-02,  8.7539e-03,  2.7704e-01,\n",
      "        -3.0792e-01, -3.9114e-01, -1.9886e-01,  3.7479e-03, -2.9536e-01,\n",
      "        -6.8354e-03, -6.0966e-03, -2.3686e-04,  2.2437e-03, -2.7310e-02,\n",
      "        -1.2685e-01, -3.3558e-01,  3.0013e-03, -2.2384e-03, -5.8274e-03,\n",
      "         1.3151e-02, -2.7389e-01,  6.7022e-03,  1.8259e-02, -5.7932e-01,\n",
      "         4.7192e-01, -3.8121e-03,  2.8352e-03,  2.4116e-01,  4.7736e-03,\n",
      "         4.3765e-02,  2.3854e-02, -3.5482e-02,  2.8927e-01, -4.3478e-01,\n",
      "        -4.0762e-01,  6.7985e-03, -3.0603e-01, -8.1102e-03,  1.5418e-01,\n",
      "         7.7581e-03, -1.0162e-03, -1.6799e-03,  1.7972e-03, -1.7475e-02,\n",
      "         4.8707e-03,  3.2061e-01,  4.0195e-04,  1.9865e-03,  2.2439e-01,\n",
      "         2.0958e-02,  5.9741e-03, -6.8900e-03,  4.6169e-01,  2.3621e-03,\n",
      "         2.2241e-02, -3.1404e-01,  3.6245e-01, -1.0008e-04,  3.2372e-01,\n",
      "        -2.1108e-03,  2.2231e-01, -9.1557e-02, -3.2795e-01, -3.6779e-02,\n",
      "        -2.6513e-01, -3.2080e-03,  1.2746e-03,  2.0681e-02,  6.6630e-01,\n",
      "         1.0428e-02, -1.5691e-02, -3.6993e-01,  4.6855e-03, -5.9871e-02,\n",
      "         1.9179e-03, -1.5194e-03,  8.2842e-03, -2.7615e-01,  6.2059e-03,\n",
      "         3.1997e-01,  2.6080e-01, -4.5782e-01, -4.3308e-04,  2.4448e-01,\n",
      "         1.5619e-03, -9.7694e-03, -1.0987e-02,  4.9571e-03,  5.4011e-03,\n",
      "         1.1235e-02,  2.1207e-01, -4.0844e-03, -2.9815e-01, -1.2998e-01,\n",
      "        -2.7168e-03,  4.0200e-03, -2.4383e-01,  2.7704e-01, -1.2861e-01,\n",
      "         7.7012e-03,  9.0119e-04,  4.0492e-01,  3.7961e-01, -9.9297e-02,\n",
      "         1.7945e-01,  6.5355e-01,  7.3524e-03,  4.0481e-01, -1.8841e-01,\n",
      "        -2.1306e-01, -3.0823e-01,  4.5744e-01,  6.9200e-01, -3.0453e-03,\n",
      "         1.2272e-02,  1.9696e-02,  9.0607e-02,  8.0042e-02, -2.1625e-05,\n",
      "        -2.7124e-01, -3.0933e-03,  3.1895e-02, -4.8977e-01,  6.6202e-03,\n",
      "        -4.5031e-01, -2.9395e-01, -9.6625e-03,  6.9767e-02,  5.2887e-03,\n",
      "         5.6380e-03, -3.7757e-01,  5.8557e-03,  3.0001e-01,  3.0601e-02,\n",
      "         9.6127e-03,  3.8086e-03,  3.1071e-01, -3.5592e-01, -1.5545e-01,\n",
      "         7.3075e-03,  4.5667e-01, -2.9625e-03, -7.1916e-04, -8.6598e-04,\n",
      "         7.8068e-03, -4.5460e-03,  6.4978e-03, -3.4595e-01, -3.0092e-01,\n",
      "         1.0015e-01, -3.8164e-01,  2.0403e-01,  1.5636e-02, -3.1936e-01,\n",
      "        -4.0312e-03, -3.3869e-01,  1.4041e-02, -3.4978e-01, -3.5980e-01,\n",
      "         2.1715e-03,  7.8858e-03,  4.6901e-01, -1.0708e-03,  5.2741e-03,\n",
      "         2.1144e-02, -1.8832e-01, -3.1313e-02,  3.5045e-01, -8.5316e-01,\n",
      "         1.9262e-02, -7.6941e-03,  4.7685e-01, -1.4078e-01,  5.6457e-01,\n",
      "         2.0985e-03,  2.6611e-01, -3.9580e-01,  4.4118e-03,  6.3210e-03,\n",
      "         2.2812e-01,  1.6400e-02,  4.8591e-02,  1.8396e-02,  7.7393e-03,\n",
      "        -1.9059e-03,  2.4329e-01,  2.0983e-01, -1.9410e-02, -1.1168e-02,\n",
      "        -7.4156e-03,  3.9895e-01, -3.5021e-02, -2.6529e-01,  1.2298e-02,\n",
      "         1.7540e-02,  6.3725e-03, -1.5857e-01,  1.1953e-02,  2.7735e-01,\n",
      "         7.7241e-03, -3.9538e-03,  2.9150e-01, -4.6429e-02,  4.2530e-02,\n",
      "        -1.6873e-02,  7.9890e-03, -1.4277e-03, -1.5058e-01, -6.4717e-03,\n",
      "         7.0204e-03, -3.3407e-01,  2.7025e-03, -1.9632e-02,  3.7108e-03,\n",
      "        -1.7742e-03, -1.7447e-03,  4.7856e-03,  2.2168e-01,  5.9467e-02,\n",
      "         1.3692e-02, -8.5619e-01, -3.9077e-01, -3.8494e-01,  2.7428e-01,\n",
      "        -1.0271e-02, -6.9894e-03, -5.4857e-01,  2.5446e-03, -3.2814e-01,\n",
      "         1.3698e-02, -4.1635e-01,  2.6116e-01, -1.1581e-01, -4.6161e-01,\n",
      "        -3.2282e-02,  1.1265e-01,  3.7133e-04, -1.8208e-01, -2.5193e-01,\n",
      "        -8.5123e-03,  2.3816e-02,  4.1836e-01, -2.4913e-01,  1.8036e-01,\n",
      "         5.0015e-01,  4.1931e-01,  3.7275e-02])\n"
     ]
    }
   ],
   "source": [
    "#print the cls token weights for jf vision_model.embeddings.class_embedding and for timm cls token\n",
    "print(clip_hf_state_dict['vision_model.embeddings.class_embedding'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.7209e-02, -5.7008e-01,  1.7252e-01, -1.9447e-03,  1.0032e+00,\n",
      "           2.3552e-02, -3.1327e-02, -1.6213e-02, -1.2675e-02, -1.6176e-01,\n",
      "           3.6407e-01, -2.0450e-01,  4.7865e-01,  3.5444e-01,  4.0919e-01,\n",
      "           1.9796e-02, -1.4939e-01,  6.0754e-03,  3.4549e-03, -4.4616e-02,\n",
      "           9.6039e-03,  1.0068e-02,  3.5203e-01, -1.5700e-03, -2.7305e-02,\n",
      "          -4.1719e-01, -9.7703e-03,  1.9158e-02,  2.8592e-03,  1.6418e-03,\n",
      "           1.1876e-02, -3.6110e-01,  3.4315e-03,  2.1195e-01, -3.3260e-01,\n",
      "          -2.4247e-01, -1.5742e-01,  1.2279e-02,  4.8328e-01, -1.0050e-01,\n",
      "           2.8371e-02, -6.9131e-02,  7.4177e-03,  3.9833e-01,  7.0305e-02,\n",
      "          -3.8821e-01,  3.1074e-03, -3.9902e-01, -2.7748e-01,  7.2899e-03,\n",
      "          -1.5799e-02,  1.3198e-02,  3.1929e-01,  3.9349e-03, -2.9630e-02,\n",
      "          -3.7958e-03,  3.2401e-01, -2.8247e-01, -4.5286e-04,  4.0700e-03,\n",
      "          -2.2941e-01, -1.0161e-02, -3.7380e-01, -2.4229e-01,  1.6912e-01,\n",
      "           7.7919e-03,  2.5575e-02,  3.4321e-01,  5.8983e-02,  8.7041e-03,\n",
      "           8.6387e-03, -2.8615e-01,  1.4049e-01, -6.9530e-02, -2.5689e-02,\n",
      "           8.6861e-03,  3.8805e-03,  4.1322e-01,  6.0722e-03,  1.2101e-01,\n",
      "           4.3213e-03,  1.4451e-02,  1.1970e-05,  1.2344e-02, -1.4839e-02,\n",
      "          -2.8091e-01, -5.5582e-04,  1.6709e-02,  9.8234e-03,  1.7198e-03,\n",
      "          -7.8114e-02,  7.3818e-03,  5.0359e-02,  4.1475e-03,  8.7621e-03,\n",
      "          -4.1586e-01,  3.2938e-04,  9.4677e-03, -3.1265e-01,  1.9122e-03,\n",
      "           4.2110e-01,  1.5032e-01,  6.4615e-03,  4.3733e-01, -1.6108e-03,\n",
      "           4.9281e-01,  2.3122e-02,  1.1459e-01,  4.1135e-04,  4.7661e-01,\n",
      "           3.5777e-02,  4.7339e-01,  1.6235e-03,  1.1110e-03, -3.5776e-02,\n",
      "          -1.8272e-01,  1.4231e-02,  1.7698e-03,  1.6046e-04,  1.8287e-02,\n",
      "          -2.0778e-03,  6.7395e-01,  6.4461e-03, -3.6283e-01,  1.1954e-02,\n",
      "          -3.7560e-01,  2.2705e-03, -2.8179e-01,  1.7413e-02, -2.4918e-01,\n",
      "          -6.6592e-03, -4.3028e-01,  3.4625e-03, -1.2824e-02, -2.6799e-01,\n",
      "           4.3592e-01,  6.1942e-03,  6.6100e-03,  1.3426e-02, -2.4855e-02,\n",
      "           2.2952e-01, -6.7899e-02, -7.6912e-03,  3.5621e-02, -2.2930e-01,\n",
      "          -3.2746e-03,  3.3737e-01,  1.0726e-02, -1.6076e-02,  3.5266e-03,\n",
      "           1.2606e-02, -3.3668e-02,  8.7779e-03, -2.7328e-01,  9.4028e-03,\n",
      "           1.1265e-02, -2.4634e-02, -4.6336e-03, -6.3863e-03,  6.2576e-03,\n",
      "           1.3848e-01, -1.2891e-01, -1.5537e+00,  1.2133e-01, -6.0076e-02,\n",
      "           1.0747e-02,  7.4486e-03,  2.4497e-01, -1.8637e-02,  5.9198e-03,\n",
      "          -3.1660e-01, -2.8404e-01, -8.7388e-03, -3.7747e-01, -1.5215e-02,\n",
      "          -2.3250e-01,  2.0439e-03,  5.0115e-04,  4.5193e-01,  5.9048e-03,\n",
      "           3.5465e-01,  3.8306e-01,  4.0577e-03,  3.6637e-01,  2.1423e-01,\n",
      "           8.3901e-03,  1.7482e-03,  4.0119e-01,  4.6774e-01,  2.5480e-02,\n",
      "          -6.8499e-02,  4.1738e-01,  2.2604e-01,  3.8059e-01,  5.8841e-03,\n",
      "          -3.6944e-01, -6.6759e-02, -4.3566e-01,  1.9618e-02,  8.3956e-03,\n",
      "          -4.7460e-01, -7.5817e-03,  5.2860e-03,  3.1420e-03,  8.3503e-04,\n",
      "           7.6339e-03,  2.4740e-01,  2.9868e-03, -7.6897e-02,  2.2088e-02,\n",
      "          -2.5523e-01, -4.9735e-01, -5.0655e-02, -4.6392e-01,  1.4133e-03,\n",
      "          -3.5439e-01, -2.7990e-01, -8.2220e-02, -2.5869e-01,  4.2214e-01,\n",
      "           1.2962e-02, -8.5387e-04, -2.4513e-02,  1.2231e-02,  8.9681e-02,\n",
      "          -3.9993e-03,  2.4617e-03, -6.7791e-03,  5.0579e-03, -3.5819e-01,\n",
      "           5.3457e-03,  3.5854e-01,  4.4830e-01,  2.9304e-01,  5.4311e-04,\n",
      "          -4.2298e-01, -3.9505e-01,  1.4610e-02,  3.8553e-03, -5.2851e-03,\n",
      "           1.6204e-01,  8.0612e-03,  8.0376e-03,  1.4297e-03,  5.3434e-03,\n",
      "          -4.2956e-02, -8.7474e-02, -4.2127e-01,  1.7239e-04,  3.7805e-01,\n",
      "           1.1972e-02, -3.7456e-01,  3.0025e-01,  1.1856e-01,  4.1934e-01,\n",
      "          -1.8370e-01, -1.0854e-02, -3.1013e-01, -1.4721e-02,  2.3574e-01,\n",
      "          -1.7547e-02,  2.6636e-02,  8.9467e-02, -2.6821e-01, -4.7130e-03,\n",
      "           5.2529e-03,  6.6395e-03, -2.6224e-01, -4.0888e-01,  5.5415e-01,\n",
      "           2.3556e-01, -1.4716e-01, -5.4020e-01,  2.5845e-03,  3.1552e-03,\n",
      "           1.0061e-02,  1.4213e-02,  4.5550e-02,  2.4636e-01, -3.2381e-01,\n",
      "           7.7000e-03,  1.4330e-02, -1.5107e-02,  2.8264e-01,  5.4835e-01,\n",
      "           1.9124e-01,  1.3763e-02, -6.7660e-03, -9.3073e-03,  9.3380e-03,\n",
      "           3.3279e-01,  1.0401e-03, -1.6795e-03,  1.3305e-02, -9.5019e-02,\n",
      "           7.9593e-03,  3.9392e-03,  2.7991e-02, -3.3914e-01,  2.1478e-03,\n",
      "          -2.8234e-01,  4.0000e-01, -3.1629e-01,  5.2064e-03, -3.0863e-01,\n",
      "           4.3077e-03,  1.7124e-02, -1.0643e-01,  2.2688e-02, -2.3652e-03,\n",
      "           5.4857e-03,  1.8332e-01,  1.3066e-02,  6.3349e-01,  9.5900e-03,\n",
      "           1.9481e-01, -3.3045e-01, -2.8378e-01,  3.3372e-03, -3.1896e-01,\n",
      "          -4.9838e-01, -4.3225e-02, -4.5054e-01,  8.6169e-03, -4.3524e-01,\n",
      "           4.0580e-03,  6.7421e-02, -3.3513e-01,  1.1705e-02,  1.0529e-01,\n",
      "           3.7387e-01,  2.0957e-01,  4.2680e-03, -1.5940e-01,  1.0806e-02,\n",
      "          -1.3836e-03,  3.5585e-03, -2.6658e-01, -4.3860e-03,  3.5356e-03,\n",
      "           4.2298e-01,  3.1948e-01,  3.5632e-02, -1.2766e-02,  4.4848e-01,\n",
      "          -3.6842e-01, -9.3655e-03,  4.1365e-01, -1.1752e-01, -2.9804e-03,\n",
      "           4.6331e-02,  4.4625e-01, -3.1980e-01, -4.9937e-03,  1.0693e-02,\n",
      "           4.7284e-03, -4.4807e-02,  3.5368e-03, -1.1446e-01, -1.8581e-01,\n",
      "          -2.9040e-01,  3.4654e-01,  2.7736e-02, -1.7870e-01,  2.5435e-04,\n",
      "          -2.1284e-01,  3.3768e-01,  1.9170e-02,  6.4218e-03,  3.6997e-02,\n",
      "           2.9352e-03, -2.1867e-01,  5.8905e-03,  3.7788e-03,  1.2010e-02,\n",
      "           8.2617e-03, -1.4471e-01, -4.4483e-03,  4.1717e-01, -1.9383e-02,\n",
      "           9.6542e-04, -1.3193e-04,  6.1482e-03,  1.9696e-03, -4.4972e-01,\n",
      "          -3.7933e-03,  9.0354e-03, -3.7821e-01,  9.9601e-02, -4.5027e-04,\n",
      "          -3.1600e-01,  4.0432e-02, -3.2853e-01, -3.5562e-02,  1.2438e-01,\n",
      "           7.0363e-03,  2.1569e-02, -1.0308e-01, -6.8448e-02,  5.8183e-03,\n",
      "           5.7580e-03,  2.1333e-02,  9.6013e-04, -1.0065e-02,  8.7419e-03,\n",
      "          -3.2981e-05,  4.8738e-01,  2.6715e-01,  3.8944e-04,  5.4350e-01,\n",
      "           5.7291e-01, -9.6718e-03, -1.4296e-01,  1.0543e-01,  1.4396e-02,\n",
      "           4.3829e-03,  5.2188e-03, -3.3270e-01, -7.1623e-03,  3.0989e-02,\n",
      "          -6.3077e-02,  8.4937e-03, -2.9535e-01, -3.0482e-02, -9.6739e-03,\n",
      "           4.1360e-01,  2.2486e-03, -3.8869e-01,  1.3115e-02,  9.7661e-01,\n",
      "           6.0808e-02,  1.9318e-01,  1.7528e-03,  1.0446e-02, -1.8814e-02,\n",
      "          -2.5829e-03,  2.6753e-02, -1.6773e-02, -3.8797e-01,  3.0209e-02,\n",
      "           5.8568e-03, -2.8913e-01, -1.2895e-05,  2.2246e-02,  1.2976e-01,\n",
      "           5.3070e-02,  3.9125e-01,  1.0076e-01, -4.6242e-03,  3.3538e-02,\n",
      "           6.2694e-03, -4.4934e-01,  5.5262e-03,  7.6566e-04, -9.2407e-05,\n",
      "          -1.0811e-02,  3.7944e-04,  6.1506e-04, -1.4525e-02,  6.6671e-03,\n",
      "           2.1837e-04, -3.4198e-04, -1.7039e-03, -5.6206e-02, -5.0602e-02,\n",
      "           3.8434e-03,  2.6947e-01, -3.5124e-01, -5.0148e-03,  6.2534e-03,\n",
      "           4.5156e-01,  1.2791e-02,  2.7110e-02, -5.9018e-03,  4.2692e-01,\n",
      "           2.0360e-01,  1.6827e-02,  1.5986e-02,  3.0807e-01,  6.0206e-01,\n",
      "          -6.3007e-02,  2.8250e-01,  4.0483e-01, -5.2749e-03,  4.2270e-01,\n",
      "           8.1791e-03, -6.2506e-03,  3.4000e-02,  4.0202e-03,  4.7591e-01,\n",
      "          -1.7815e-03, -5.7022e-03, -5.8992e-03,  2.9111e-04,  3.2199e-01,\n",
      "           3.9326e-01, -2.9738e-03,  4.2645e-04,  8.3516e-03,  2.6811e-01,\n",
      "           4.2667e-03,  7.2611e-03, -3.5387e-01,  1.9313e-03, -3.0769e-03,\n",
      "          -2.4823e-01, -3.8411e-02, -8.3992e-02, -5.9510e-03,  1.2335e-02,\n",
      "          -2.2880e-01,  8.7755e-03, -2.2040e-03,  6.5197e-01, -3.0416e-01,\n",
      "           2.7271e-03, -6.5202e-03, -2.5392e-02, -4.6386e-01,  4.8087e-03,\n",
      "           8.9364e-03,  5.3590e-01, -2.2904e-02,  8.7539e-03,  2.7704e-01,\n",
      "          -3.0792e-01, -3.9114e-01, -1.9886e-01,  3.7479e-03, -2.9536e-01,\n",
      "          -6.8354e-03, -6.0966e-03, -2.3686e-04,  2.2437e-03, -2.7310e-02,\n",
      "          -1.2685e-01, -3.3558e-01,  3.0013e-03, -2.2384e-03, -5.8274e-03,\n",
      "           1.3151e-02, -2.7389e-01,  6.7022e-03,  1.8259e-02, -5.7932e-01,\n",
      "           4.7192e-01, -3.8121e-03,  2.8352e-03,  2.4116e-01,  4.7736e-03,\n",
      "           4.3765e-02,  2.3854e-02, -3.5482e-02,  2.8927e-01, -4.3478e-01,\n",
      "          -4.0762e-01,  6.7985e-03, -3.0603e-01, -8.1102e-03,  1.5418e-01,\n",
      "           7.7581e-03, -1.0162e-03, -1.6799e-03,  1.7972e-03, -1.7475e-02,\n",
      "           4.8707e-03,  3.2061e-01,  4.0195e-04,  1.9865e-03,  2.2439e-01,\n",
      "           2.0958e-02,  5.9741e-03, -6.8900e-03,  4.6169e-01,  2.3621e-03,\n",
      "           2.2241e-02, -3.1404e-01,  3.6245e-01, -1.0008e-04,  3.2372e-01,\n",
      "          -2.1108e-03,  2.2231e-01, -9.1557e-02, -3.2795e-01, -3.6779e-02,\n",
      "          -2.6513e-01, -3.2080e-03,  1.2746e-03,  2.0681e-02,  6.6630e-01,\n",
      "           1.0428e-02, -1.5691e-02, -3.6993e-01,  4.6855e-03, -5.9871e-02,\n",
      "           1.9179e-03, -1.5194e-03,  8.2842e-03, -2.7615e-01,  6.2059e-03,\n",
      "           3.1997e-01,  2.6080e-01, -4.5782e-01, -4.3308e-04,  2.4448e-01,\n",
      "           1.5619e-03, -9.7694e-03, -1.0987e-02,  4.9571e-03,  5.4011e-03,\n",
      "           1.1235e-02,  2.1207e-01, -4.0844e-03, -2.9815e-01, -1.2998e-01,\n",
      "          -2.7168e-03,  4.0200e-03, -2.4383e-01,  2.7704e-01, -1.2861e-01,\n",
      "           7.7012e-03,  9.0119e-04,  4.0492e-01,  3.7961e-01, -9.9297e-02,\n",
      "           1.7945e-01,  6.5355e-01,  7.3524e-03,  4.0481e-01, -1.8841e-01,\n",
      "          -2.1306e-01, -3.0823e-01,  4.5744e-01,  6.9200e-01, -3.0453e-03,\n",
      "           1.2272e-02,  1.9696e-02,  9.0607e-02,  8.0042e-02, -2.1625e-05,\n",
      "          -2.7124e-01, -3.0933e-03,  3.1895e-02, -4.8977e-01,  6.6202e-03,\n",
      "          -4.5031e-01, -2.9395e-01, -9.6625e-03,  6.9767e-02,  5.2887e-03,\n",
      "           5.6380e-03, -3.7757e-01,  5.8557e-03,  3.0001e-01,  3.0601e-02,\n",
      "           9.6127e-03,  3.8086e-03,  3.1071e-01, -3.5592e-01, -1.5545e-01,\n",
      "           7.3075e-03,  4.5667e-01, -2.9625e-03, -7.1916e-04, -8.6598e-04,\n",
      "           7.8068e-03, -4.5460e-03,  6.4978e-03, -3.4595e-01, -3.0092e-01,\n",
      "           1.0015e-01, -3.8164e-01,  2.0403e-01,  1.5636e-02, -3.1936e-01,\n",
      "          -4.0312e-03, -3.3869e-01,  1.4041e-02, -3.4978e-01, -3.5980e-01,\n",
      "           2.1715e-03,  7.8858e-03,  4.6901e-01, -1.0708e-03,  5.2741e-03,\n",
      "           2.1144e-02, -1.8832e-01, -3.1313e-02,  3.5045e-01, -8.5316e-01,\n",
      "           1.9262e-02, -7.6941e-03,  4.7685e-01, -1.4078e-01,  5.6457e-01,\n",
      "           2.0985e-03,  2.6611e-01, -3.9580e-01,  4.4118e-03,  6.3210e-03,\n",
      "           2.2812e-01,  1.6400e-02,  4.8591e-02,  1.8396e-02,  7.7393e-03,\n",
      "          -1.9059e-03,  2.4329e-01,  2.0983e-01, -1.9410e-02, -1.1168e-02,\n",
      "          -7.4156e-03,  3.9895e-01, -3.5021e-02, -2.6529e-01,  1.2298e-02,\n",
      "           1.7540e-02,  6.3725e-03, -1.5857e-01,  1.1953e-02,  2.7735e-01,\n",
      "           7.7241e-03, -3.9538e-03,  2.9150e-01, -4.6429e-02,  4.2530e-02,\n",
      "          -1.6873e-02,  7.9890e-03, -1.4277e-03, -1.5058e-01, -6.4717e-03,\n",
      "           7.0204e-03, -3.3407e-01,  2.7025e-03, -1.9632e-02,  3.7108e-03,\n",
      "          -1.7742e-03, -1.7447e-03,  4.7856e-03,  2.2168e-01,  5.9467e-02,\n",
      "           1.3692e-02, -8.5619e-01, -3.9077e-01, -3.8494e-01,  2.7428e-01,\n",
      "          -1.0271e-02, -6.9894e-03, -5.4857e-01,  2.5446e-03, -3.2814e-01,\n",
      "           1.3698e-02, -4.1635e-01,  2.6116e-01, -1.1581e-01, -4.6161e-01,\n",
      "          -3.2282e-02,  1.1265e-01,  3.7133e-04, -1.8208e-01, -2.5193e-01,\n",
      "          -8.5123e-03,  2.3816e-02,  4.1836e-01, -2.4913e-01,  1.8036e-01,\n",
      "           5.0015e-01,  4.1931e-01,  3.7275e-02]]])\n"
     ]
    }
   ],
   "source": [
    "print(clip_timm_state_dict['cls_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "      (position_embedding): Embedding(197, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_hf.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_timm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_image = torch.randn(1, 3, 384, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.9666e-05)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass with both models\n",
    "with torch.no_grad():\n",
    "    output_hf = clip_hf(pixel_values=random_image, interpolate_pos_encoding=True)\n",
    "    output_timm = clip_timm(random_image)\n",
    "a = output_hf.pooler_output - output_timm\n",
    "torch.mean(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, \\\n",
    "                         AutoModelForImageClassification, \\\n",
    "                         CLIPVisionModel, CLIPModel, CLIPTokenizerFast, \\\n",
    "                         default_data_collator\n",
    "\n",
    "from sample4geo.dataset.vigor_plus import VigorCombinedDataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "text_processor = CLIPTokenizerFast.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=348, scale=(0.08, 1), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.AutoAugment(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "transform_val = transforms.Compose([\n",
    "        transforms.Resize((348, 348)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "dataset_root = '/home/erzurumlu.1/yunus/research_drive/data/VIGOR'\n",
    "cities_to_include_train = ['NewYork', 'Chicago']\n",
    "cities_to_include_val = ['Seattle', 'SanFrancisco']\n",
    "train_dataset = VigorCombinedDataset(\n",
    "    dataset_root,\n",
    "    cities_to_include_train,\n",
    "    transform=transform_train\n",
    ")\n",
    "val_dataset = VigorCombinedDataset(\n",
    "    dataset_root,\n",
    "    cities_to_include_val,\n",
    "    transform=transform_val,\n",
    "    val=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPProcessor\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def collate_fn(examples):\n",
    "    # images = [example[0] for example in examples]\n",
    "    # text = [example[1] for example in examples]\n",
    "    # text = text_processor(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # text['interpolate_pos_encoding'] = True\n",
    "    # text['pixel_values'] = torch.stack(images)\n",
    "    # text['return_loss'] = True\n",
    "    images = [example[0] for example in examples]\n",
    "    text = [example[1] for example in examples]\n",
    "    inputs = clip_processor(images=None, text=text, return_tensors='pt',\n",
    "                            padding=True, truncation=True)\n",
    "    inputs['pixel_values'] = torch.stack(images)\n",
    "    inputs['interpolate_pos_encoding'] = True\n",
    "    inputs['return_loss'] = True\n",
    "\n",
    "    return inputs\n",
    "a = collate_fn([train_dataset[0], train_dataset[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/home/erzurumlu.1/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='2560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/2560 00:43 < 15:22:40, 0.05 it/s, Epoch 0.02/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m CLIP\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch16\u001b[39m\u001b[38;5;124m\"\u001b[39m,vision_config\u001b[38;5;241m=\u001b[39mvision_config)\n\u001b[1;32m     41\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     42\u001b[0m                   args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[1;32m     43\u001b[0m                     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     44\u001b[0m                     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m     45\u001b[0m                     data_collator\u001b[38;5;241m=\u001b[39mcollate_fn,\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/transformers/trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3612\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[1;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/accelerate/accelerator.py:2241\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLIP-LoRA/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "train_args = TrainingArguments(\n",
    "        output_dir='/home/erzurumlu.1/yunus/research_drive/language_pretrain',\n",
    "        overwrite_output_dir = True,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=50,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        gradient_accumulation_steps=8, # 8 for 4 GPUs\n",
    "        learning_rate=1e-06,\n",
    "        weight_decay=0.001,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.98,\n",
    "        adam_epsilon=1e-06,\n",
    "        max_grad_norm=1.0,\n",
    "        num_train_epochs=20,\n",
    "        max_steps=-1,\n",
    "        lr_scheduler_type = 'linear',\n",
    "        warmup_ratio = 0.2,\n",
    "        logging_first_step = False,\n",
    "        logging_steps=1,\n",
    "        save_strategy='steps',\n",
    "        save_steps=50,\n",
    "        seed=42,\n",
    "        dataloader_drop_last=True,\n",
    "        run_name=None,\n",
    "        adafactor=False,\n",
    "        report_to='tensorboard',\n",
    "        skip_memory_metrics=True,\n",
    "        resume_from_checkpoint=None,\n",
    "    )\n",
    "from transformers import CLIPVisionConfig\n",
    "vision_config = CLIPVisionConfig.from_pretrained(\"openai/clip-vit-base-patch16\", hidden_act = \"gelu\")\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\",vision_config=vision_config)\n",
    "class CLIP(transformers.models.clip.modeling_clip.CLIPModel):\n",
    "    def forward(self, **kwargs):\n",
    "        return super().forward(**kwargs, interpolate_pos_encoding=True, return_loss=True)\n",
    "model = CLIP.from_pretrained(\"openai/clip-vit-base-patch16\",vision_config=vision_config)\n",
    "trainer = Trainer(model=model,\n",
    "                  args=train_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=val_dataset,\n",
    "                    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPOutput(loss=None, logits_per_image=tensor([[24.5420, 24.5525]], grad_fn=<TBackward0>), logits_per_text=tensor([[24.5420],\n",
       "        [24.5525]], grad_fn=<MulBackward0>), text_embeds=tensor([[ 0.0148,  0.0070, -0.0234,  ..., -0.0508, -0.0438,  0.0033],\n",
       "        [ 0.0087,  0.0258, -0.0387,  ..., -0.0547, -0.0242,  0.0112]],\n",
       "       grad_fn=<DivBackward0>), image_embeds=tensor([[-8.2083e-03,  3.4670e-03,  3.6016e-02, -2.5022e-03,  3.0891e-03,\n",
       "          2.3843e-03, -5.4070e-03,  6.8715e-02,  1.9265e-02,  2.3204e-02,\n",
       "          1.8708e-02,  1.3606e-02, -2.6386e-02, -4.6457e-02,  1.0047e-02,\n",
       "          1.2466e-03, -2.9311e-03,  6.6638e-03,  6.7595e-04,  1.1112e-02,\n",
       "          2.0449e-02, -3.5057e-02, -1.2712e-02,  2.8116e-02,  5.1621e-03,\n",
       "         -2.6811e-03,  3.8826e-04,  1.8637e-02,  5.0991e-02, -2.6290e-02,\n",
       "          5.7535e-02, -1.5716e-02, -2.1326e-02,  3.8808e-02, -4.2637e-03,\n",
       "         -6.4914e-02,  2.1025e-02,  6.8399e-03, -4.7344e-03, -1.1451e-01,\n",
       "         -4.1007e-03,  1.6164e-03,  1.2346e-02, -7.9285e-03, -1.4623e-02,\n",
       "          1.1971e-01, -1.6384e-02, -4.9946e-03,  3.7110e-02, -9.9346e-03,\n",
       "         -2.3606e-02, -7.9587e-03,  3.6484e-02,  1.3128e-02,  1.2889e-02,\n",
       "         -1.8530e-02, -2.7898e-02,  2.8413e-03, -8.5976e-03, -2.4840e-02,\n",
       "          4.9936e-02, -1.1078e-02, -2.9628e-02, -3.5103e-03, -2.5114e-02,\n",
       "          4.3382e-02, -5.0331e-02, -4.2360e-02, -1.5972e-02,  4.4664e-02,\n",
       "         -2.3494e-02, -2.8069e-03,  4.9575e-02,  2.0335e-02, -2.9486e-02,\n",
       "         -4.0770e-02,  3.2660e-02,  3.8282e-02, -4.3738e-03, -1.6185e-02,\n",
       "         -1.8805e-03,  3.6031e-02, -2.4771e-03,  1.3680e-03, -4.9326e-03,\n",
       "          7.7890e-02,  5.3794e-02, -2.3625e-02,  6.1200e-04, -2.0363e-02,\n",
       "          3.5207e-02, -1.7489e-02, -7.0364e-01,  3.3469e-02,  8.8623e-05,\n",
       "          2.8575e-05,  4.8958e-02, -2.3785e-02,  2.0519e-02, -3.8401e-02,\n",
       "          1.1464e-02, -2.5816e-02, -3.1300e-02, -2.4940e-03, -3.8571e-02,\n",
       "          2.6140e-02, -1.1232e-01,  6.0217e-04, -1.5066e-02,  2.9483e-02,\n",
       "         -5.9206e-04, -9.6718e-03,  4.1005e-03,  2.3076e-03,  2.7251e-02,\n",
       "         -3.9515e-02, -2.4227e-02, -9.6736e-03,  5.1931e-04,  5.7590e-02,\n",
       "          3.8665e-04, -7.0100e-02, -2.7179e-02, -3.1594e-03, -2.8255e-02,\n",
       "          1.8069e-02, -7.0298e-03,  3.3162e-02,  3.8902e-02, -3.2843e-03,\n",
       "         -2.1712e-02,  1.4080e-03, -4.0632e-02,  9.6613e-02,  5.5324e-02,\n",
       "          7.6129e-02, -3.2769e-02, -7.1727e-02, -6.5017e-02,  1.2488e-03,\n",
       "         -3.4957e-02,  2.0035e-02,  1.5922e-03, -4.0128e-02, -2.2455e-02,\n",
       "         -1.2150e-03, -9.8645e-04, -2.8739e-02,  1.3539e-02, -5.7485e-03,\n",
       "         -4.8102e-05,  2.3183e-02,  3.5660e-02,  1.3845e-02, -2.7484e-02,\n",
       "         -4.5623e-02,  4.7679e-02,  1.6375e-02, -1.4712e-02,  2.9569e-02,\n",
       "         -2.7899e-02, -9.7435e-03, -8.2290e-03, -1.0399e-02,  7.1400e-03,\n",
       "          5.7202e-02, -1.1534e-03, -4.3330e-03, -6.3939e-02, -3.3783e-02,\n",
       "         -6.7937e-03,  1.4673e-03, -4.2123e-02, -4.9512e-04,  6.1571e-03,\n",
       "         -1.7865e-03, -3.3174e-02, -2.1988e-02,  2.4128e-02, -3.7758e-02,\n",
       "         -6.1251e-02,  1.3213e-02, -4.0541e-02, -2.7465e-04,  2.4971e-03,\n",
       "         -1.5464e-02,  3.7546e-02,  2.2931e-02,  6.4759e-03,  3.7643e-02,\n",
       "         -2.6634e-03, -2.8414e-02,  1.9616e-02, -2.2089e-02,  2.8276e-02,\n",
       "          4.4971e-03,  1.4046e-03, -1.3638e-02, -3.7624e-02,  1.7983e-02,\n",
       "          3.5321e-04, -1.8326e-02,  2.5292e-02, -4.7310e-03, -3.0354e-02,\n",
       "         -2.3994e-02,  4.4861e-03,  2.7183e-02, -7.7932e-03,  5.8964e-03,\n",
       "         -1.4012e-02,  8.7243e-02, -3.1844e-02, -2.8542e-02,  4.6061e-03,\n",
       "         -1.7725e-02, -1.7393e-02, -1.4138e-02, -3.1719e-02, -5.8284e-02,\n",
       "          4.7152e-02,  1.2775e-02, -1.2832e-02,  1.2856e-02,  5.1898e-02,\n",
       "         -1.9897e-02,  3.0870e-02,  4.4465e-03, -1.5431e-03, -1.4219e-02,\n",
       "         -3.4608e-02,  1.7658e-02, -1.2606e-02, -5.9749e-02, -5.8310e-03,\n",
       "          8.8948e-03, -7.4680e-03, -5.2332e-02, -1.2946e-02,  1.9160e-02,\n",
       "         -2.3894e-02, -5.2516e-03,  5.8982e-02, -8.1883e-03, -2.3665e-03,\n",
       "         -2.6313e-02, -8.2202e-03,  9.5866e-03,  3.7354e-02, -1.6744e-03,\n",
       "          2.4071e-02, -2.1100e-02, -5.9609e-03,  4.0964e-02,  4.5853e-02,\n",
       "         -2.7662e-02, -7.2756e-03,  3.1723e-02, -6.8728e-02, -3.1793e-02,\n",
       "          5.7869e-03, -1.3214e-02,  1.3072e-02, -8.5159e-03, -2.5301e-02,\n",
       "          2.4702e-02,  4.9217e-03, -2.8123e-02, -5.1920e-02,  3.2186e-02,\n",
       "          2.4950e-03, -1.1079e-05,  3.7961e-03,  1.3226e-02, -1.8314e-02,\n",
       "          5.4779e-02, -6.9945e-03,  3.9135e-02, -6.2874e-03,  2.2907e-02,\n",
       "         -3.7693e-02,  3.3918e-02,  6.2102e-02,  2.8771e-02,  2.5264e-02,\n",
       "          1.2525e-02,  2.7578e-02,  3.8887e-03,  5.0356e-02, -1.8579e-02,\n",
       "         -1.0588e-02,  7.6749e-03,  3.1546e-02,  1.9582e-02,  2.3902e-02,\n",
       "          5.3232e-02, -8.3195e-02, -1.5381e-02,  3.7236e-02, -3.1664e-02,\n",
       "          4.2748e-02,  1.5246e-03, -9.5451e-03, -3.1293e-02, -2.0630e-02,\n",
       "          5.2765e-03,  3.4620e-02,  9.9430e-03, -7.4611e-03,  1.8548e-02,\n",
       "          3.2919e-02,  6.4330e-03,  9.6499e-02,  1.4938e-03, -3.1141e-02,\n",
       "          3.9321e-02,  2.7094e-02,  3.8633e-02,  2.0578e-02, -2.2009e-02,\n",
       "          1.4950e-02,  1.8338e-01,  1.2710e-02,  3.8104e-02, -1.2462e-02,\n",
       "         -2.3247e-02, -1.8645e-02, -1.6235e-02,  8.1346e-03,  3.4216e-02,\n",
       "         -1.5424e-02,  2.9725e-02, -1.6152e-02, -1.9800e-02, -1.0024e-02,\n",
       "         -2.8410e-02,  1.1613e-02, -2.5856e-02, -1.3564e-02, -3.7274e-03,\n",
       "          2.7355e-02, -1.9400e-03, -1.0466e-02,  2.6709e-02, -7.3404e-03,\n",
       "         -1.3309e-02,  1.1940e-02, -1.1280e-02,  3.4740e-02, -1.2189e-02,\n",
       "          3.3751e-02, -1.1523e-02, -4.5346e-02,  5.5894e-03,  3.3743e-02,\n",
       "          1.7669e-02,  7.1748e-03, -2.0738e-02,  5.7343e-03, -1.8106e-02,\n",
       "          2.1127e-02,  5.7641e-03,  8.5107e-03, -2.6212e-02, -7.3537e-02,\n",
       "          3.3582e-03,  6.6703e-03,  4.1185e-02,  4.0688e-02,  1.2676e-02,\n",
       "         -8.5945e-03, -2.5459e-02,  9.8783e-03,  3.6452e-03,  1.5603e-02,\n",
       "          6.4238e-03, -2.3128e-02,  3.4625e-02, -8.7021e-03, -1.1349e-02,\n",
       "          2.4499e-02, -2.2372e-02,  1.4727e-02,  2.1958e-02, -7.4658e-03,\n",
       "         -1.4404e-02,  3.8646e-02, -1.3034e-02,  2.3980e-02, -6.7592e-02,\n",
       "         -2.3735e-02, -2.7770e-04,  1.7894e-02,  2.3564e-02,  1.0648e-02,\n",
       "          6.0772e-02,  4.8703e-02,  2.8324e-03, -4.8731e-03, -5.2572e-02,\n",
       "         -4.3678e-02, -4.1747e-02, -3.0643e-02,  1.0746e-02,  1.8641e-02,\n",
       "          1.6613e-02,  3.7877e-02,  5.1209e-02,  1.9173e-02,  1.1517e-03,\n",
       "          3.5897e-02, -2.4879e-02, -1.5672e-02, -3.3001e-03,  4.6732e-02,\n",
       "         -2.4811e-02, -1.2909e-02, -6.2333e-02, -9.5047e-03, -3.2124e-03,\n",
       "          2.4521e-02, -2.3502e-03, -5.1200e-02, -4.2243e-02,  1.7743e-02,\n",
       "          3.4997e-02,  6.6246e-03,  2.0897e-02,  4.7953e-02,  1.0875e-02,\n",
       "         -1.6669e-02, -2.9579e-02, -5.0029e-02,  1.2061e-02,  8.8086e-03,\n",
       "          5.3592e-02,  1.9146e-02, -4.2831e-02, -1.5693e-02,  1.2027e-02,\n",
       "          3.5580e-02, -4.9888e-02, -5.3482e-03, -1.2535e-02,  3.3667e-02,\n",
       "         -3.0114e-02,  4.0107e-02,  4.7948e-03,  3.9771e-02, -8.7028e-02,\n",
       "         -4.3142e-02,  6.6115e-03,  2.6375e-02, -1.0630e-02,  2.7696e-02,\n",
       "         -1.1407e-02, -6.5289e-02, -3.4061e-02,  5.5484e-04, -5.6695e-02,\n",
       "         -1.5429e-02,  3.4627e-02,  3.5844e-02, -8.7402e-03,  1.8444e-02,\n",
       "         -3.6106e-02,  2.5206e-02,  1.4125e-02,  1.1688e-02,  1.5937e-02,\n",
       "         -1.8549e-04, -4.0702e-02, -1.8666e-02, -7.0246e-02, -5.4322e-03,\n",
       "          2.2536e-02,  1.3298e-03, -1.8575e-02, -9.9508e-03,  2.8606e-02,\n",
       "         -2.4375e-03, -2.0560e-02, -3.7337e-02,  4.9036e-02, -2.0467e-02,\n",
       "         -1.1292e-02,  3.3626e-02,  1.4403e-02, -2.6401e-02, -1.4333e-02,\n",
       "         -9.6000e-03, -2.9339e-02,  2.3271e-02, -3.7784e-02,  9.5596e-03,\n",
       "          1.5800e-02,  4.0459e-03, -3.0840e-02, -3.9820e-02, -2.3391e-02,\n",
       "          3.8085e-02, -8.1526e-03,  1.2976e-02,  1.0523e-02,  3.9562e-02,\n",
       "          4.0476e-02, -1.7859e-02, -3.0995e-04,  8.7740e-03,  2.4607e-02,\n",
       "         -3.8813e-02,  1.1966e-02]], grad_fn=<DivBackward0>), text_model_output=BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "         [ 1.0118, -0.6701,  1.7742,  ..., -0.1556, -0.0250, -1.5062],\n",
       "         [-0.5152,  0.1658,  0.8876,  ..., -0.0675, -0.4551, -1.7960]],\n",
       "\n",
       "        [[ 0.3393,  0.1165,  0.1020,  ...,  0.2468,  0.5906,  0.1013],\n",
       "         [ 1.9753, -0.5844,  0.3685,  ...,  1.1658,  0.8050, -0.9801],\n",
       "         [ 1.0580, -0.9600,  1.0018,  ..., -0.5155, -0.1437, -1.9444],\n",
       "         ...,\n",
       "         [ 0.3059, -1.5037, -0.4022,  ..., -0.0224,  0.9105, -0.3916],\n",
       "         [-0.1433, -0.5163,  1.7099,  ..., -0.0795,  0.3609, -1.2437],\n",
       "         [ 0.0426,  0.0189,  1.2740,  ..., -0.4217, -0.4393, -1.3016]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.5152,  0.1658,  0.8876,  ..., -0.0675, -0.4551, -1.7960],\n",
       "        [ 0.0426,  0.0189,  1.2740,  ..., -0.4217, -0.4393, -1.3016]],\n",
       "       grad_fn=<IndexBackward0>), hidden_states=None, attentions=None), vision_model_output=BaseModelOutputWithPooling(last_hidden_state=tensor([[[ 0.7159,  0.1507, -0.0765,  ..., -0.0791,  0.4214, -0.0872],\n",
       "         [-0.2584,  0.1371, -0.5831,  ...,  0.0320,  0.3191,  0.3678],\n",
       "         [ 0.1231,  0.3240, -0.2203,  ...,  0.2299,  0.3870,  0.1002],\n",
       "         ...,\n",
       "         [ 0.0057,  0.3623, -0.4270,  ...,  0.1930,  0.7309,  0.1544],\n",
       "         [-0.2618,  0.4936, -0.5301,  ...,  0.0064,  0.5610, -0.0534],\n",
       "         [-0.0452,  0.7701, -0.2354,  ...,  0.4465,  0.6570,  0.2564]]],\n",
       "       grad_fn=<AddBackward0>), pooler_output=tensor([[ 2.2985e+00,  9.8179e-02, -5.8482e-02,  2.0816e-01,  1.9798e+00,\n",
       "         -7.0358e-01,  1.8152e+00,  2.0014e+00,  6.9040e-01,  1.3927e+00,\n",
       "         -3.8026e-01, -9.2461e-01,  4.6754e-01,  1.1952e-01,  3.1565e-01,\n",
       "          2.3856e-02,  2.2112e+00,  6.8979e-01,  2.6407e-01,  3.6105e-01,\n",
       "         -3.7835e-01, -3.4996e-01,  1.7796e+00,  1.6182e+00, -1.0677e+00,\n",
       "          1.6952e+00,  8.7799e-01,  6.0288e-01, -6.6518e-01,  4.9951e-01,\n",
       "         -9.1700e-01, -1.3514e-01,  1.8359e-02, -3.6231e-01,  4.8317e-01,\n",
       "         -2.5998e-01, -4.2098e-01, -6.1852e-03, -2.9419e-01, -1.4130e-01,\n",
       "         -3.4971e-01,  3.3923e-01,  2.4846e-01,  4.6947e-01, -1.8617e-01,\n",
       "          2.0427e-01, -7.5890e-01,  1.2823e+00,  1.3633e+00, -2.4726e-01,\n",
       "          2.3823e-01, -8.0992e-01, -7.3299e-01, -3.6813e-01, -6.2031e-01,\n",
       "         -1.9040e-01, -2.2773e-01,  8.5700e-01,  2.4927e-01, -2.0169e-01,\n",
       "          5.1235e-01, -8.8495e-01, -6.3217e-01, -4.4346e-01,  4.7152e-01,\n",
       "          6.3233e-02,  4.7408e-02, -4.4878e-01, -3.7615e-01,  7.3497e-02,\n",
       "         -3.6391e-01,  1.2541e+00,  3.3227e-01,  4.3342e-01, -3.9737e-02,\n",
       "          5.5506e-01,  8.1253e-01,  1.6743e-02, -2.0998e-01,  7.6090e-01,\n",
       "          7.9680e-01, -6.1658e-01, -1.4449e-01,  6.1771e-01,  6.0053e-01,\n",
       "          1.4605e-01, -6.4952e-01, -6.5262e-02,  8.7232e-01,  7.1967e-01,\n",
       "          9.5663e-02,  3.6818e-01,  7.9563e-01,  1.3259e+00, -1.0887e+00,\n",
       "          1.2020e+00, -1.6997e-01, -1.2681e+00,  2.7166e-01, -6.0064e-01,\n",
       "          6.8197e-01,  1.7823e+00, -1.6797e+00,  1.2431e+00, -2.3993e-01,\n",
       "          1.8539e-01, -6.8501e-02,  4.4435e-01, -7.0108e-02, -3.2329e-02,\n",
       "         -3.4086e-02,  3.5051e-01,  1.8153e-01, -7.3719e-02, -6.3249e-01,\n",
       "         -1.2699e+00, -8.7859e-01,  1.9165e-01,  3.1412e-01,  7.5322e-01,\n",
       "          1.3425e+00, -1.3515e-01,  4.1885e-02, -3.2001e-01, -1.8295e+00,\n",
       "          1.0315e+00,  7.7155e-01, -3.2103e-01, -3.8299e+00,  3.5650e-01,\n",
       "         -8.0715e-01,  4.8776e-01,  6.1977e-01,  8.4239e-01,  3.7379e-01,\n",
       "          4.3192e-01,  1.3848e+00,  7.7749e-01,  9.9338e-01,  2.8998e-01,\n",
       "          3.1193e-01, -1.5412e-01,  2.8222e-01, -8.9332e-01,  6.9796e-01,\n",
       "          1.3954e-01,  2.5845e-01, -2.8458e-01,  1.4005e-01,  3.6444e-01,\n",
       "         -7.6974e-01, -4.9827e-02, -5.1038e-01, -2.3293e-01,  7.7268e-01,\n",
       "          1.0518e+00, -5.4816e-01,  1.3198e-01, -1.2878e+00, -3.5675e-01,\n",
       "          1.2052e-01,  4.3290e-01,  1.7161e-01, -7.6216e-01,  2.9048e-01,\n",
       "          6.7224e-01,  6.8716e-01,  1.4670e+00,  1.0310e-01, -8.4625e-01,\n",
       "          4.3219e-01, -1.0866e+00, -3.0453e-01, -7.8660e-01, -7.7671e-01,\n",
       "          1.3861e+00,  1.3480e+00,  8.8951e-01, -4.6234e-01,  1.8123e+00,\n",
       "         -9.1964e-01,  5.9991e-01, -6.8834e-01,  7.4445e-01,  2.6307e-01,\n",
       "         -1.7662e-01, -9.6668e-01, -4.8172e-01, -1.5018e-03, -2.1069e-01,\n",
       "         -2.1194e-01, -1.1413e+00, -7.0090e-01,  8.3484e-01,  9.4404e-01,\n",
       "          3.1365e-01, -6.3799e-01,  1.7024e+00, -2.5710e-01, -1.7717e-03,\n",
       "          3.9101e-01,  1.9276e+00, -1.3895e+00,  2.1890e-01,  5.9569e-02,\n",
       "         -5.3408e-01,  7.4748e-01,  2.2337e+00, -2.7095e-01, -4.2101e-01,\n",
       "         -2.5292e-01,  1.1461e+00,  5.0788e-01, -5.9211e-01,  8.6642e-01,\n",
       "          1.5335e+00, -5.4796e-01,  9.6074e-01,  4.8246e-01,  1.6573e+00,\n",
       "         -2.5121e-01, -8.5064e-01, -7.2491e-01,  9.8860e-01, -3.7126e-01,\n",
       "         -8.0524e-01,  8.3782e-01,  6.0316e-01, -1.9457e-01, -4.9142e-01,\n",
       "          9.0847e-01,  2.2158e-01,  2.5588e+00, -6.7322e-01, -6.6087e-01,\n",
       "         -2.5637e-01,  4.2691e-01, -5.0993e-01,  7.0372e-01,  3.9285e-01,\n",
       "         -3.4686e-01, -4.4819e-01,  6.3983e-01,  8.4845e-02,  1.3907e-01,\n",
       "         -8.8979e-01,  1.7551e-01,  5.2140e-01,  7.0661e-01, -1.9141e-01,\n",
       "          5.0169e-02, -3.1026e-01,  1.8810e-01,  6.0391e-01,  4.2540e-02,\n",
       "         -1.9531e+00,  1.4693e+00,  2.9091e-02, -4.9752e-01,  7.6529e-01,\n",
       "         -1.3291e+00,  5.0792e-01,  1.1510e+00,  7.4610e-01,  6.5217e-01,\n",
       "          2.7003e-02, -7.5160e-01, -8.6898e-01,  1.6477e+00, -6.5749e-01,\n",
       "          8.0721e-01,  1.7117e+00,  9.8806e-01, -1.7863e+00,  2.7197e-02,\n",
       "         -5.3263e-02,  1.2159e+00,  7.5875e-01, -5.1567e-01,  3.0882e-01,\n",
       "         -3.0858e-01,  1.0768e+00,  1.1055e+00, -6.1128e-01, -3.3131e-01,\n",
       "         -5.6537e-01, -7.3659e-01,  3.3371e-01,  1.7922e+00, -4.6447e-01,\n",
       "         -9.2145e-01,  5.7732e-01, -5.3629e-02,  1.2749e-01,  7.3009e-01,\n",
       "         -5.8977e-01, -1.8920e-01,  1.1404e+00,  1.6523e+00,  5.3621e-01,\n",
       "          4.2027e-01, -1.2150e-01, -1.0531e+00,  2.3519e+00, -3.4327e-01,\n",
       "         -3.5403e-01, -3.4078e-01,  4.5649e-01, -1.6373e-01,  2.9871e+00,\n",
       "         -1.0752e+00,  1.8878e-01, -6.2599e-01,  4.7928e-01, -1.4818e+00,\n",
       "          6.9447e-01, -2.6265e-01,  1.4394e+00, -2.9594e-01,  9.6808e-01,\n",
       "          9.5094e-01,  5.9352e-01,  2.2968e-01,  7.7691e-01,  1.0496e+00,\n",
       "          1.3089e+00, -3.3636e-01, -2.0637e-01, -2.6816e-01, -5.3739e-01,\n",
       "          1.9703e+00,  9.6443e-01,  6.0285e-01,  5.7310e-01, -3.3425e-01,\n",
       "          1.0968e-01, -3.4266e+00, -6.1948e-01,  2.4431e-01,  1.6785e-01,\n",
       "          5.5308e-01,  1.5900e+00, -4.8637e-01,  8.1369e-02,  1.8838e+00,\n",
       "         -5.4189e-01, -6.4800e-01,  3.1141e-01, -6.1619e-02,  4.9457e-01,\n",
       "          1.8257e+00,  1.7606e-01, -1.5013e+00, -6.2914e-01, -7.0621e-01,\n",
       "          2.5911e-01,  2.7713e-01, -1.3296e+00,  1.2098e-01,  1.1145e-01,\n",
       "          4.8986e-01,  5.7624e-01,  3.7013e-01, -3.9395e-01,  7.1497e-01,\n",
       "          9.1320e-01,  3.4051e-01, -1.1600e+00, -1.6918e+00,  1.0452e+00,\n",
       "         -1.1210e+00, -1.5641e+00,  4.0991e-02, -1.7293e-01, -7.1185e-01,\n",
       "         -2.6378e-01,  1.1293e+00,  2.0969e-01, -7.8974e-01, -9.8810e-02,\n",
       "          7.3316e-01, -6.4080e-01,  1.1720e+00, -6.5776e-01,  9.0471e-01,\n",
       "         -9.8826e-01,  2.5645e-01,  6.2136e-01,  1.1147e+00, -1.1951e+00,\n",
       "          5.4781e-01,  5.2335e-02, -3.9304e-01, -2.5443e-01,  6.2617e-02,\n",
       "          1.2164e+00,  1.8929e+00,  6.9523e-02,  1.4158e+00,  1.0613e+00,\n",
       "          3.7037e-01,  3.2786e-01,  1.3733e-01,  5.6320e-01,  5.5515e-01,\n",
       "          4.6423e-01, -4.7930e-01,  5.7151e-01,  9.6699e-01,  5.4545e-01,\n",
       "          5.2960e-01,  2.5815e-01, -1.2465e+00, -7.3828e-01,  4.7605e-01,\n",
       "          6.1335e-01,  2.6307e-01,  2.8801e+00,  4.8501e-03, -2.3317e-01,\n",
       "          3.1809e-01,  4.6727e-01,  4.3992e-01, -5.8809e-01,  6.6104e-01,\n",
       "         -1.2119e-01, -2.7836e-01,  1.3051e+00,  2.0600e-01,  6.2554e-01,\n",
       "          1.4851e+00,  1.0160e+00,  1.7006e-01,  1.9088e-02, -1.4505e-01,\n",
       "          9.8050e-02,  2.1768e+00, -6.2153e-01,  8.8726e-01, -3.8605e-01,\n",
       "         -1.0406e+00, -1.1838e+00,  9.7132e-01,  1.2370e-01, -1.9396e+00,\n",
       "          8.9994e-01,  8.0627e-01,  1.4352e+00,  7.0356e-01, -1.2116e-01,\n",
       "          1.0057e+00,  9.6537e-02,  1.3144e+00, -6.0397e-02,  7.0082e-01,\n",
       "         -1.4513e+00,  3.5972e-01,  2.9034e-01,  4.6725e-02,  3.1328e-01,\n",
       "          7.3091e-01,  1.7061e+00, -1.6321e+00,  1.0570e+00, -5.4525e-01,\n",
       "          4.7799e-01, -8.2242e-01, -4.9205e-01,  4.3105e-01, -9.4688e-01,\n",
       "          1.1398e+00,  7.0206e-01,  9.8463e-01, -3.9051e-01, -6.0685e-01,\n",
       "         -4.2059e-01, -1.1042e+00, -1.8804e-01,  1.6789e-01,  1.2412e+00,\n",
       "          2.5050e-01,  1.0347e+00, -1.2477e+00,  1.1087e+00,  1.0960e+00,\n",
       "         -4.4297e-01,  1.9125e+00, -9.0221e-01, -8.8519e-01,  7.5064e-01,\n",
       "          1.6473e+00,  9.6223e-01,  8.1173e-01,  1.1519e+00,  2.0214e+00,\n",
       "          1.3185e+00, -4.6898e-01, -3.3721e-01,  3.3082e-01,  8.2062e-01,\n",
       "          1.4315e+00, -3.3338e-01,  1.0795e+00, -9.2421e-01,  1.1515e-01,\n",
       "          1.1416e+00,  4.2232e-01, -6.4163e-01,  1.8117e+00,  3.9909e-01,\n",
       "          5.1706e-01,  3.7598e-01, -1.6288e+00, -6.2873e-01,  1.5715e+00,\n",
       "          8.4123e-01,  3.1454e-02,  7.1734e-01,  1.4966e+00,  5.8466e-01,\n",
       "         -5.5804e-01,  4.1485e-01, -4.4762e-01, -2.2801e-01, -5.1389e-01,\n",
       "          4.1598e-02,  6.2708e-01,  3.5040e-01, -6.2084e-01,  1.1391e+00,\n",
       "          9.7632e-01,  4.8220e-01,  2.9060e-01, -5.8171e-01, -1.9378e+00,\n",
       "          8.4059e-01,  1.5037e+00, -3.6159e-01,  3.1245e-01, -1.0454e+00,\n",
       "          4.7731e-01,  2.0835e+00, -3.4867e-01,  1.0130e+00,  1.2608e+00,\n",
       "         -6.4608e-01,  1.1291e+00, -4.7545e-01,  6.4977e-01,  1.1131e+00,\n",
       "          6.4501e-01,  7.7173e-01, -1.1001e+00, -5.0713e-01, -4.6654e-01,\n",
       "          3.3084e-01, -3.8362e-01, -1.5531e-01,  5.1919e-01, -9.9594e-01,\n",
       "         -6.2373e-01,  1.7570e-01,  2.4806e-02,  6.3710e-01, -1.1592e-01,\n",
       "         -3.6462e-01, -2.0560e-01,  4.8567e-01, -7.9223e-01,  1.0452e-01,\n",
       "         -1.4960e+00,  6.4237e-01,  1.0238e-01,  5.7819e-01,  5.3240e-01,\n",
       "          5.2742e-01, -2.3559e-01,  5.4624e-01,  1.3583e+00,  9.3819e-01,\n",
       "          9.1800e-01, -1.3467e-01,  1.5319e+00,  2.1346e-01,  6.3827e-01,\n",
       "          1.1132e+00,  5.0437e-02, -2.1241e+00,  3.3083e-01,  1.2559e-01,\n",
       "          5.1196e-01,  9.9688e-01,  8.8840e-01, -6.5911e-01, -8.4046e-01,\n",
       "          1.3630e-02,  2.5021e-01,  6.1286e-01, -7.7862e-01, -6.2553e-01,\n",
       "          6.4382e-01,  1.3892e+00, -6.5106e-01,  3.1392e-01,  3.6746e+00,\n",
       "         -8.4543e-01,  1.2696e+00,  1.3709e-02, -6.7208e-02, -2.7659e-01,\n",
       "         -9.9079e-01,  4.1863e-01,  1.7063e+00,  1.5303e-01,  1.5882e-01,\n",
       "          1.7518e+00,  1.9830e+00,  1.5052e-01,  4.3415e-01,  4.6135e-01,\n",
       "         -9.9019e-01,  9.3434e-02,  1.2380e-01,  2.1288e+00, -8.8596e+00,\n",
       "         -1.5663e+00,  2.2339e-01, -2.6284e-01,  2.2643e-01, -9.1701e-01,\n",
       "         -4.0937e-01, -5.0578e-01, -4.9592e-02,  1.3226e+00,  1.3900e+00,\n",
       "         -1.1198e+00,  5.9365e-01, -1.8829e-01,  2.6029e-01, -3.6820e-01,\n",
       "          8.2123e-01, -4.6110e-01,  3.0282e-01,  1.3885e-01,  4.4248e-01,\n",
       "         -9.0203e-01,  4.9157e-01, -1.8210e-01,  4.9841e-01, -5.8004e-01,\n",
       "          1.2343e+00, -1.8869e-01, -9.4503e-01, -6.6096e-02,  2.1112e-01,\n",
       "          8.3984e-01,  4.2641e-01,  1.6938e+00,  9.7446e-01,  8.9273e-01,\n",
       "         -2.0897e-01,  1.5607e+00,  1.2911e-01,  1.2244e-01, -3.7066e-01,\n",
       "          1.0699e+00, -1.3995e+00,  1.2362e+00,  3.1561e-01, -6.3057e-01,\n",
       "          4.0503e-01,  5.7452e-01, -7.3968e-01, -2.9246e-01,  7.2994e-01,\n",
       "         -1.1305e+00, -2.1793e-01,  1.3342e+00,  2.2531e-01, -1.3091e-01,\n",
       "          9.5600e-01, -5.9440e-01,  7.5939e-01, -1.1093e-01,  1.6751e+00,\n",
       "          6.0436e-01, -7.4217e-01, -9.0082e-01,  2.4567e+00,  1.4412e+00,\n",
       "          7.9306e-01,  3.1837e-01,  2.3782e-01, -1.1104e+00, -3.6230e-01,\n",
       "         -5.9810e-01,  9.0184e-01,  5.8035e-02, -5.4682e-01, -6.2398e-01,\n",
       "          6.4312e-01,  2.9788e-01,  7.5033e-02,  6.3159e-01,  1.1395e+00,\n",
       "         -9.1320e-01,  3.4441e-01,  6.4662e-01,  1.4030e+00, -2.2412e-01,\n",
       "          1.4243e-01, -1.4107e-01, -3.4368e-02, -8.5083e-01,  8.6677e-01,\n",
       "          7.0629e-01, -6.3372e-01, -1.0066e+00, -1.1122e+00,  3.6780e-01,\n",
       "          9.5839e-02, -1.2520e+00,  1.0456e+00,  2.3946e-01,  1.3969e-01,\n",
       "         -1.3305e+00, -4.7403e-01,  1.5263e+00, -1.7196e-02,  8.9215e-01,\n",
       "         -1.0969e+00,  2.2104e-02,  7.0081e-01, -7.5661e-02, -8.5513e-01,\n",
       "          5.1075e-01,  3.7311e-01,  2.0070e+00,  1.4248e+00,  2.6468e-01,\n",
       "          5.2952e-01,  2.2726e+00,  2.0217e+00,  1.1823e+00,  1.1259e+00,\n",
       "         -2.4822e-01, -4.3825e-01,  8.9771e-01, -6.5552e-01, -1.0440e+00,\n",
       "          2.1196e-01, -7.2289e-01,  3.1287e+00,  1.3171e+00, -1.4632e-01,\n",
       "          9.4539e-01,  6.8939e-01,  1.5917e+00,  6.9340e-01,  7.4745e-01,\n",
       "          3.4059e+00,  1.0421e+00, -1.8082e-01,  1.2970e-01,  1.1757e+00,\n",
       "         -2.5701e-01,  1.4810e+00, -8.8816e-02]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "image = torch.randn(1, 3, 384, 384)\n",
    "inputs['pixel_values'] = image\n",
    "inputs['interpolate_pos_encoding'] = True\n",
    "model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP-LoRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
